\section{Variations in presentation}

\subsection{A little more history}

If one were to reify the notion in $\Scala$ there are several design
choices -- all of which endure some desiderata. Following the original
presentation developed in category theory, however, has some crucial
advantages:

\begin{itemize}
  \item intuition
  \item correspondence to previously existing structures
  \item decomposition of the requirements 
\end{itemize}

which we explore in some detail here.

\subsubsection{Intuition: Monad as container}

As we will see the notion of monad maps nicely onto an appropriately
parametric notion of container. From this point of view we can imagine
a container ``API'' that has three basic operations. 
\paragraph{Shape of the container} The first of these is a
\emph{parametric} specification of the \emph{shape} of the
container. Examples of container shapes include: \lstinline[language=Scala]!List[A]!,
\lstinline[language=Scala]!Set[A]!, \lstinline[language=Scala]!Tree[A]!, etc. At the outset we remain
uncommitted to the particular shape. The API just demands that
there is some shape, say \lstinline[language=Scala]!S[A]!.
\paragraph{Putting things into the container} The next operation is
very basic, it says how to put things into the container. To align
with a very long history, we will refer to this operation by the name
\lstinline[language=Scala]!unit!. Since the operation is supposed to allow us to put
elements of type \lstinline[language=Scala]!A! into containers of shape \lstinline[language=Scala]!S[A]!, we
expect the signature of this operation to be \lstinline[language=Scala]!unit : A => S[A]!.
\paragraph{Flattening nested containers} Finally, we want a generic
way to flatten nested containers. Just like there's something
fundamentally the same about the obvious way to flatten nested lists
and nested sets, we ask that the container API provide a canonical way
to flatten nested containers. If you think about it for a moment, if a
container is of shape, \lstinline[language=Scala]!S[A]!, then a nested container will be
of shape, \lstinline[language=Scala]!S[S[A]]!. If history demands that we call our
flattening operation \lstinline[language=Scala]!mult!, then our generic flatten operation
will have signature, \lstinline[language=Scala]!mult : S[S[A]] => S[A]!.

\subsubsection{Preserving connection to existing structure: Monad as
  generalization of monoid}

Programmers are very aware of data structures that support a kind of
concatenation operation. The data type of \lstinline[language=Scala]!String! is a perfect
example. Every programmer expects that the concatenation of a given
\lstinline[language=Scala]!String!, say \lstinline[language=Scala]!s!, with the empty \lstinline[language=Scala]!String!,
\lstinline[language=Scala]!""! will return a result string equal to the original. In
code, \lstinline[language=Scala]! s.equals( s + "" ) == true !. Likewise, string
concatenation is insensitive to the order of operation. Again, in
code, \lstinline[language=Scala]! (( s + t ) + u).equals( s + ( t + u ) ) == true !.

Most programmers have noticed that these very same laws survive
polymorphic interpretations of \lstinline[language=Scala]!+!, \lstinline[language=Scala]!equals! and the
``empty'' element. For example, if we substituted the data type
\lstinline[language=Scala]!Integer! as the base type and used integer addition, integer
equality, and \lstinline[language=Scala]!0! as the empty element, these same code
snippets (amounting assertions) would still work.

Many programmers are aware that there is a very generic underlying
data type, historically referred to as a \emph{monoid} defined by
these operations and laws. In code, we can imagine defining a
\lstinline[language=Scala]!trait! in $\Scala$ something like

\begin{lstlisting}[language=Scala]
  trait Monoid {
    def unit : Monoid
    def mult( that : Monoid ) 
  }
\end{lstlisting}

This might allow \emph{views} of \lstinline[language=Scala]!Int! as a monoid as in

\begin{lstlisting}[language=Scala]
  class MMultInt extends Int with Monoid {
    override def unit = 1
    override def mult( that : Monoid ) = this * that
    }
\end{lstlisting}

except for the small problem that \lstinline[language=Scala]!Int! is
\lstinline[language=Scala]!final! (illustrating an important
difference between the adhoc polymorphism of \texttt{Haskell}'s
\lstinline[language=Haskell]!typeclass! and \texttt{Scala}'s
\lstinline[language=Scala]!trait!).

Any solution will depend on type parametrization. For example

\begin{lstlisting}[language=Scala]
  trait Monoid[Element] {
    def unit : Element
    def mult( a : Element, b : Element ) 
  }
\end{lstlisting}

and corresponding view of \lstinline[language=Scala]!Int! as a monoid.

\begin{lstlisting}[language=Scala]
  class MMultInt extends Monoid[Int] {
    override def unit : Int = 1
    override def mult( a : Int , b : Int ) = a * b
  }
\end{lstlisting}

This parametric way of viewing some underlying data structure is
natural both to the modern programmer and the modern
mathematician. Both are quite familiar with and make extensive use of
overloading of this kind. Both are very happy to find higher levels of
abstraction that allow them to remain DRY when the programming demands
might cause some perspiration. One of the obvious places where
repetition is happening is in the construction of view. Consider
another view of \lstinline[language=Scala]!Int!

\begin{lstlisting}[language=Scala]
  class MAddInt extends Monoid[Int] {
    override def unit : Int = 0
    override def mult( a : Int , b : Int ) = a + b
  }
\end{lstlisting}

It turns out that there is a lot of machinery that is common to
defining a view like this for any given data type. Category theorists
realized this and recognized that you could reify the \emph{view}
which not only provides a place to refactor the common machinery, but
also to give it another level of polymorphism. Thus, a category
theorist's view of the monad API might look something like this.

\begin{lstlisting}[language=Scala]
  trait Monad[Element,M[_]] {
    def unit( e : Element ) : M[Element]
    def mult( mme : M[M[Element]] ) : M[Element] 
  }
\end{lstlisting}

The family resemblance to the \lstinline[language=Scala]!Monoid! API
is not accidental. The trick is to bring syntax back into the picture. Here's an example.

\begin{lstlisting}[language=Scala]
  case class MonoidExpr[Element]( val e : List[Element] )
  class MMInt extends Monad[Int,MonoidExpr] {
    override def unit( e : Int ) = MonoidExpr( List( e ) )
    override def mult( mme : MonoidExpr[MonoidExpr[Int]] ) =
    mme match {
      case MonoidExpr( Nil ) =>
         MonoidExpr( Nil )
      case MonoidExpr( mes ) => 
         MonoidExpr(
            ( Nil /: mes)( 
               { ( acc, me ) => me match { 
                   case MonoidExpr( es ) => acc +++ es 
                 } 
               } 
             )
         )
    }
  }
\end{lstlisting}

While it's clear that \lstinline[language=Scala]!unit! turns
\lstinline[language=Scala]!Int!s into integer expressions, what the
operation \lstinline[language=Scala]!mult! is doing is canonically
flattening nested expressions in a way the exactly parallels the
flattening of nest arithmetic addition expressions. For a broad class
of monads, this is the paradigmatic behavior of
\lstinline[language=Scala]!mult!. The fact that monads are
characterized by a generic interpretation of flattening of nested
structure, by the way, makes the choice of the term
\lstinline[language=Scala]!flatMap! particularly appropriate.

\paragraph{Associativity as flattening}
Looking at it from the other way around, one of the properties of a
monoid is that it's binary operation, its
\lstinline[language=Scala]!mult!, is associative. The actual content of
the notion of associativity is that order of grouping doesn't make any
difference. In symbols, a binary operation, $*$, is associative when
$a*(b*c) = (a*b)*c$. This fact gives us the right to erase the parens
and simply write $a*b*c$. In other words, associativity is
flattening. A similar connection can be made for
\lstinline[language=Scala]!unit! and the identity of a monoid. One
quick and dirty way to see this is that since we know that $a*e=a$
(when $e$ is the unit of the monoid) then the expression $a*e$
effectively nests $a$ in a
\lstinline[language=Scala]!MonoidExpr!. That's the ``moral'' content
of the connection between the two notions of unit.

\paragraph{Bracing for \texttt{XML}}
In this connection it is useful to make yet another connection to a
ubiquitous technology, namely \texttt{XML}. As a segue, notice that we
can always write a binary operation in prefix notation as well as
infix. That is, whatever we could write at $a*b$ we could just as
easily write as $*(a,b)$. The flattening property of associativity
says we can drop nesting such as $*(a,*(b,c))$ in favor of
$*(a,b,c)$. In this sense, the syntax of braces is a kind of generic
syntax for monoids and monads. If we introduce the notion of
``colored'' braces, this becomes even more clear at the lexicographic
or notational level. So, instead of $*(a,b,c)$ we'll mark the
``color'' of the braces like so: $(*| ... |*)$, where $*$ can be any
color. Then, at the level of monoid the unit is the empty braces, $(*|
|*)$, while at the level of the monad the unit places the element, say
$a$, in between the braces: $(*| a |*)$. The conceptual connection
between the two variations of the operation now becomes clear: writing
$a*e$ is the same as writing $*(a,e)$ which is the same as writing
$(*| a , (*| |*) |*)$, which canonically flattens into $(*| a |*)$.

Now, anyone who's spent any time around \texttt{XML} can see where
this is headed. At a purely syntactic, lexicographic level we replace
round brackets with angle brackets and we have exactly \texttt{XML}
notation for elements. In this sense, \texttt{XML} is a kind of
universal notation for monads. The only thing missing from the
framework is a means to associate operations to unit and mult, i.e. to
inserting content into elements and flattening nested
elements. \texttt{Scala}'s specific support for \texttt{XML} puts it
in an interesting position to rectify this situation.

\paragraph{The connection with set-comprehensions}
Finally, since we've gone this far into it, we might as well make the
connection to comprehensions. Again, let's let notation support our
intuitions. The above discussion should make it clear that its not the
particular shape of the brace that matters, but the action of
``embracing'' a collection of elements that lies at the heart of the
notion. So, it's fine if we shift to curly braces to be
suggestive. Thus, we are looking at a formalism that allows us to
polymorphically ``collect'' elements between braces, like $\{*| a, b, c |*\}$.

This is fine for finite collections, but what about infinitary
collections or collections of elements selected programmatically,
rather than given explicitly. The set theoretic notation was designed
specifically for this purpose. When we have an extant set of elements
that we can give explicitly, we simply write $\{ a_1, a_2, a_3, ... \}$.
When we have a potentially infinitary collection of elements, or
elements that are selected on the basis of a condition, then we write
$\{ pattern \in S \; | \; condition \}$. The idea of monad as comprehension
recognizes that these operations of collecting, pattern matching and
selection on the basis of a condition can be made \emph{polymorphic}
using monads. Notationally, we can denote the different polymorphic
interpretations by the ``color'' of the brace. In other words, we are
looking at a shift of the form
\begin{itemize}
  \item $\{ a_1, a_2, a_3, ... \} \mapsto \{*| \; a_1, a_2, a_3, ... \; |*\}$
  \item $\{ pattern \; \in \; S \;|\; condition \} \mapsto \{*| \; pattern \in S \; | \; condition \; |*\}$
\end{itemize}
to build into our notation an explicit representation of the fact that
the operation of collection, pattern matching and filtering on the
basis of predicate are polymorphic. \footnote{ This demarcation
  between extensionally and intensionally given expressions is also
  reflected in the notation used for arithmetic or monoids, more
  generally. When we have a finite number and/or explicitly given set
  of operands, we can write expressions like $a_1 + a_2 + ... + a_n$,
  but when we have an infinite expression (like and infinite series)
  or an expression whose operands are given programmatically we write
  expressions like $\displaystyle\sum\limits_{i \in S}^{} e( i )$. }

Often times, good mathematics, like good programming is really about
the design of good notation -- it's about DSLs! In this case, the
notation is particularly useful \emph{because} it begs the question of the
language of patterns and the language of conditions -- something that
Wadler's original paper on monads as generalized comprehensions did
not address. This is a theme to which we will return at the end of the
book when we address search on a semantics basis. For now, the central
point is to understand how monad as container and monad as
generalization of monoid are actually views of the same underlying
idea.

Now, just to make sure the connection is absolutely explicit, there is
a one-for-one correspondence between the polymorphic set-comprehension
notation and the \lstinline[language=Scala]!for!-comprehension
notation of \texttt{Scala}. The correspondence takes
$\{*| pattern \in S \; | \; condition |*\}$ to

\begin{lstlisting}[language=Scala,mathescape=true]
  for( x <- $S$ if $condition$ ) yield {
    x match { case $pattern$ => x }
  }
\end{lstlisting}

As the \texttt{Scala} type checker will explain, this translation is
only approximate. If the pattern is refutable, then we need to handle
the \lstinline[language=Scala]!case! when the match is not
possible. Obviously, we just want to throw those away, so a
\lstinline[language=Scala]!fold! might be a better a better choice,
but then that obscures the correspondence.

\paragraph{Syntax and containers}
The crucial point in all of this is that \emph{syntax is the only
  container we have for computation}. What do we mean by this? Back
when Moggi was crafting his story about the application of the notion
of monad to computing he referred to monads as ``notions of
computation''. What he meant by that was that monads reify computation
(such as I/O or flow of control or constructing data structures) into
``objects''. Computation as a phenomenon, however, is both dynamic and
(potentially) infinitary. At least as we understand it today, it's not
in the category of widgets we can hold in our hand like an apple or an
Apple $\texttrademark$ computer. All we can do is \emph{point} to it,
indicate it in some way. Syntax, it turns out, is our primary means of
signifying computation. That's why many monads factor out as a
reification of syntax, and why they are so key to DSL-based design.

\subsubsection{Decomposition of monad requirements}

In the presentation of the monad API that we've discussed here the
constraints on any given monad candidate are well factored into three
different kinds of requirements -- operating at different levels of
the ``API'', dubbed in order of abstraction: functoriality, naturality
and coherence. Often these can be mechanically verified, and when they
can't there are natural ways to generate spot-checks that fit well
with tools such as $\ScalaCheck$.

\subsubsection{A categorical way to look at monads}

One of the principle challenges of presenting the categorical view of
monads is the dependencies on the ambient theory. In some sense the
categorical view of the monad API is like a useful piece of software
that drags in a bunch of other libraries. A complete specification of
monad from the categorical point of view requires providing
definitions for

\begin{itemize}
  \item category
  \item functor
  \item natural transformation
\end{itemize}

This book is not intended to be a tutorial on category theory. There
are lots of those and Google and Wikipedia are your friends. Rather,
this book is about a certain design pattern that can be expressed, and
originally was expressed within that theory, but is to a great extent
an independent notion. \footnote{In point of fact, at present writing,
  i suspect that there is a way to turn category theory on its head
  and make the notion of monad as the fundamental building block out
  of which the rest of category theory may be defined.} On the other
hand, for the diligent and/or curious reader a pointer to that body of
work has the potential to be quite rewarding. There are many treasures
there waiting to be found. For our purposes, we strike a
compromise. We take the notion of category to be given in terms of the
definable types within the \texttt{Scala} type system and the
definable programs (sometimes called maps) between those types. Then a
functor, say \lstinline[language=Scala,mathescape=true]!F!, is a pair
consisting of a parametric type constructor,
\lstinline[language=Scala,mathescape=true]!F$_T$!, together with a
corresponding action, say
\lstinline[language=Scala,mathescape=true]!F$_M$!, on programs, that
respects certain invariants. Specifically,

\begin{itemize}
\item A functor must preserve identity. That is, for any type,
  \lstinline[language=Scala,mathescape=true]!A!, we can define an
  identity map, given canonically by the program
  \lstinline[language=Scala,mathescape=true]!( x : A ) => x!. Then
  \lstinline[language=Scala,mathescape=true]!F$_M$( ( x : A ) => x ) = ( x : F$_T$[A] ) => x!
  \item A functor must preserve composition. That is, given two programs, \lstinline[language=Scala,mathescape=true]!f : A => B! and \lstinline[language=Scala,mathescape=true]!g : B => C!, \lstinline[language=Scala,mathescape=true]!F$_M$( f $\circ$ g ) = F$_M$( f ) $\circ$ F$_M$( g )! where \lstinline[language=Scala,mathescape=true]! ( f $\circ$ g )( x ) = g( f( x ) )!
\end{itemize}

In \texttt{Scala}-land this is what it means for
\lstinline[language=Scala,mathescape=true]!F = (F$_T$,F$_M$)! to be
\emph{functorial}. The constraint itself is called
\emph{functoriality}. Sometimes we will refer to the tuple
\lstinline[language=Scala,mathescape=true]!(F$_T$,F$_M$)! just by
\lstinline[language=Scala,mathescape=true]!F! when there is little
risk of confusion.

From these operational definitions, it follows that a natural
transformation is map between functors! We expect it to be given in
terms of component maps. That is, at a type, say
\lstinline[language=Scala,mathescape=true]!A!, a natural
transformation, \lstinline[language=Scala,mathescape=true]!n! from a
functor \lstinline[language=Scala,mathescape=true]!F! to a functor
\lstinline[language=Scala,mathescape=true]!G! should have a map
\lstinline[language=Scala,mathescape=true]!n$_A$ : F$_T$[A] => G$_T$[A]!.
These component maps need to satisfy some constraints. To wit,

\begin{itemize}
  \item Suppose we have a map \lstinline[language=Scala,mathescape=true]!f : A => B!. Then we want \lstinline[language=Scala,mathescape=true]!n$_A$ $\circ$ G$_M$( f ) = F$_M$( f ) $\circ$ n$_B$!.
\end{itemize}

As you might have guessed, this constraint is dubbed
\emph{naturality}. Category theorists have developed a nice
methodology for reasoning about such constraints. They draw them as
diagrams. For example, the diagram below represents the naturality
equation.

\begin{diagram}
  F_T[A] & \rTo^{n_A} & G_T[A] \\
  \dTo^{F_M(f)} & & \dTo_{G_M(f)} \\
  F_T[B] & \rTo_{n_B} & G_T[B]
\end{diagram}

You can read the diagram as stating that the two paths from the upper left
corner of the diagram to the lower right corner (one along the top and
down the right and the other down the left and along the bottom) must
be equal as functions. In general, when all the path between two
vertices in such a diagram are equal as functions, the diagram is said
to commute. This sort of tool is really invaluable for people doing
systems-level design.

\paragraph{Quick digression about design by diagram-chasing}
In preparation for this book i was looking at the open source
\texttt{Scala} library, \texttt{kiama}. This package provides support
for rewrite systems, making it possible to give a specification of
systems like the $lambda$-calculus at a level more closely resembling
its original specification. That system makes the choice of using
\texttt{Scala} \lstinline[language=Scala,mathescape=true]!case!
classes as the input language for the terms in rewrite clauses. i
wanted to use it over the \texttt{Java} classes generated from a
parser written in \texttt{BNFC}. This meant i needed a
\lstinline[language=Scala,mathescape=true]!case! class shadow of the
\texttt{BNFC}-generated \texttt{Java}-based abstract syntax model. It
just turns out that \texttt{BNFC} also generates a \texttt{DTD}-driven
\texttt{XML} parser for element structure that is isomorphic to the
input grammar. There are open source tools that will generate an
\texttt{XSD} schema from the \texttt{DTD}. This can then be fed into
the open source tool \texttt{scalaxb} which will generate
\lstinline[language=Scala,mathescape=true]!case! classes. In pictures


\begin{diagram}
  EBNF & \rTo^{BNFC} & DTD & \rTo^{trang} & XSD & \rTo^{scalaxb} case \; classes
%  \dTo^{F_M(f)} & & \dTo_{G_M(f)} \\
%  F_T[B] & \rTo_{n_B} & G_T[B]
\end{diagram}

Chaining through the open source components (maps in our category) to
find a way to wire in the \texttt{kiama} functionality is a lot like
diagram chasing, which feels like it was made for an open source
world. Moreover, when \texttt{BNFC} eventually targets \texttt{Scala}
directly, we have a quality assurance constraint. Up to some accepted
variance in output format we want

\begin{diagram}[tight]
  EBNF & \rTo^{BNFC}    & DTD & \rTo^{trang} & XSD \\
       & \rdTo^{BNFC++}(3,2) &     &              & \dTo_{scalaxb} \\
       &                &     &              & case \; classes \\
\end{diagram}

Often developers will draw similar diagrams, intuitively attempting to
convey similar information; but, just as often, because of their
informality, these diagrams are just as much a source of
mis-communication. It is possible, however, in a language such as
\texttt{Scala} to get both more formal and more intuitive (by
appealing to a higher level of abstraction, like diagram-chasing) at
the same time.

\paragraph{Monads are triples}
Returning to the topic at hand, a monad is really given by a
triple\footnote{In fact, in the early days of category theory they
  were actually give the imaginative moniker: triple.},
\lstinline[language=Scala,mathescape=true]!(S, unit, mult)! where

\begin{itemize}
  \item \lstinline[language=Scala,mathescape=true]!S! is a functor, 
  \item \lstinline[language=Scala,mathescape=true]!unit! is a natural transformation from the identity functor to \lstinline[language=Scala,mathescape=true]!S!, 
  \item \lstinline[language=Scala,mathescape=true]!mult! is a natural transformation from \lstinline[language=Scala,mathescape=true]!S$^2$! to \lstinline[language=Scala,mathescape=true]!S!.
\end{itemize}

subject to the following constraints.

\begin{itemize}
\item \lstinline[language=Scala,mathescape=true]!mult $\circ$ S mult = mult $\circ$ mult S! %[mult-mult]
\item \lstinline[language=Scala,mathescape=true]!mult $\circ$ S unit = mult $\circ$ unit S! %[mult-unit]
\end{itemize}

Or in pictures

\begin{tabular}{cc}
\begin{diagram}
  S^3 & \rTo^{S \; mult} & S^2 \\
  \dTo^{mult \; S} & & \dTo_{mult} \\
  S^2 & \rTo_{mult} & S
\end{diagram} &
\begin{diagram}
  S & \rTo^{unit \; S} & S^2 \\
  \dTo^{S \; unit} & & \dTo_{mult} \\
  S^2 & \rTo_{mult} & S
\end{diagram}
\end{tabular}

which are really shorthand for

\begin{tabular}{cc}
\begin{diagram}
  S[S[S[A]] & \rTo^{S( mult_A )} & S[S[A]] \\
  \dTo^{mult_{S[A]}} & & \dTo_{mult_A} \\
  S[S[A]] & \rTo_{mult_A} & S[A]
\end{diagram} &
\begin{diagram}
  S[A] & \rTo^{unit_{S[A]}} & S[S[A]] \\
  \dTo^{S(unit_A)} & & \dTo_{mult_A} \\
  S[S[A]] & \rTo_{mult_A} & S[A]
\end{diagram}
\end{tabular}

These constraints are called coherence constraints because they ensure
that \lstinline[language=Scala,mathescape=true]!unit! and
\lstinline[language=Scala,mathescape=true]!mult! interact coherently
(and, in fact, that \lstinline[language=Scala,mathescape=true]!mult!
interacts coherently with itself).

\texttt{Scala} programmers can certainly understand these laws. Once
you observe that nesting of containers corresponds to iterated
invocation of the functor associated with a monad, then it's easy to
see that the first diagram merely expresses that there is a canonical
way to flatten nested monadic structure. The second diagram says that
whichever way you try to nest with the
\lstinline[language=Scala,mathescape=true]!unit! transform,
applying the \lstinline[language=Scala,mathescape=true]!mult! after
results in the same flattened structure.

Despite the apparent complexity, the presentation has a certain
organization to it that is very natural once it becomes clear. There
are actually three different levels in operation here, following a kind
of food-chain. 
\begin{itemize}
\item At the level of \texttt{Scala}, which -- if you recall
-- is our ambient category, we find types and maps between
them. 
\item Though this is harder to see because we have restricted our view
to just \emph{one} category, at the level of functors,
\emph{categories} play in the role of types, while \emph{functors}
play in the role of maps between them.
\item At the level of natural
transformations, functors play in the role of types while natural
transformations play in the role of maps between them.
\end{itemize}

Correspondingly, we have three different levels of constraints.
\begin{itemize}
  \item functoriality
  \item naturality
  \item coherence
\end{itemize}

Monads bring all three levels together into one package. Monads
operate on a category via a functor and pair of natural
transformations that interact coherently. This food chain arrangement
points the way toward an extremely promising reconstruction of the
notion of interface. One way to think about it is in terms of the
recent trend away from inheritance and towards composition. In this
trend the notion of interface is still widely supported, but it really
begs the question: what is an interface? What makes a collection of
functions cohere enough to be tied together under an interface?

One way to go about answering that question is to assume there's
nothing but the interface name that collects the functions it gathers
together. In that case, how many interfaces are there? One way to see
that is just to consider all the sub interfaces of a single interface
with $n$ methods on it: that's $2^n$ interfaces. That's a
lot. Does that give us any confidence that any one way of carving up
functionality via interfaces is going to be sane? Further, in
practice, do we see random distribution through this very large space?

What we see over and over again in practice is that the answer to
the latter question is ``no!'' Good programmers invariably pick out just a
few factorizations of possible interfaces -- from the giant sea of
factorizations. That means that there is something in the mind of a
good programmer that binds a collection of methods together. What
might that something be? i submit that in their minds there are some
constraints they know or at least intuit must hold across these
functions. The evidence from category theory is that these are not
just arbitrary constraints, but that the space of constraints that
bind together well factored interfaces is organized along the lines of
functoriality, naturality and coherence. There may yet be higher-order
levels of organization beyond that, but these -- at least -- provide a
well-vetted and practical approach to addressing the question of what
makes a good interface. If monad is the new object, then these sorts
of categorical situations (of which monad is but one instance) are the
basis for a re-thinking what we mean when we say ``interface''.

All of this is discussion leads up to the context in which to
understand the correspondence between the \texttt{Haskell} variation
of the monad laws and their original presentation. 

% These two definitions are interchangeable. That's what makes them
% ``presentations'' of the same underlying idea.

% One of the reasons for the difference in presentation is that $\Haskell$
% doesn't treat the Monad type class as a Functor. The refactoring of
% the $mult$ map into the $bind$ map is that it builds functoriality
% into definition. The other reason is the do notation.






